{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hack Night at MSFT - May 20, 2025\n",
        "\n",
        "![Image](https://images.lumacdn.com/cdn-cgi/image/format=auto,fit=cover,dpr=2,background=white,quality=75,width=100,height=100/event-covers/gu/98d09ce6-a834-45bb-b82b-f5d91c629d84.png)\n",
        "\n",
        "Today, we're going to explore observability over our RAG Applications. [Weaviate](https://weaviate.io/) provides the retrieval, [FriendliAI](https://friendli.ai/) provides the inference layer, and [Comet Opik](https://comet.com/opik) is our observability layer.\n",
        "\n",
        "This simple example will get you started with using Opik, Weaviate, and Friendli Serverless Endpoints to build a RAG system.\n",
        "\n",
        "To use this notebook successfully, you'll need an account with Comet, Friendli and Weaviate.\n",
        "\n",
        "\n",
        "**Note:** A Weaviate cluster is already set up, so you technically don't need to create a new cluster, and you can just READ off an existing cluster. If you want to learn more about how this cluster was set up, check out the `weaviate-embeddings-and-friendliai` dierctory in [this repository](https://github.com/weaviate/BookRecs/tree/main/data-pipeline).\n",
        "\n",
        "You can create free accounts on all platforms.\n"
      ],
      "metadata": {
        "id": "w4lyGVV413Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up your Environment with Comet Opik\n",
        "\n",
        "[Comet](https://www.comet.com/) provides a hosted version of the Opik platform, simply [create a free account](https://www.comet.com/site/products/opik/) and grab you API Key from the UI."
      ],
      "metadata": {
        "id": "pQl7CHhX3XBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need pip install the opik and openai libraries."
      ],
      "metadata": {
        "id": "KQdYMW091iz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U opik openai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063DSqmmqKQM",
        "outputId": "37532e10-94ef-4e0d-dccb-466beeaf94ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.2/547.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll configure Opik and FriendliAI with our respective API keys."
      ],
      "metadata": {
        "id": "D238f3uZ1mPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "\n",
        "opik.configure(use_local=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7uYVtVMqNhs",
        "outputId": "2359872a-d5c0-4129-be24-943a4cab4858"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Your Opik API key is available in your account settings, can be found at https://www.comet.com/api/my/settings/ for Opik cloud\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Opik API key:··········\n",
            "Do you want to use \"ramchandra3101\" workspace? (Y/n)Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FriendliAI Inference\n",
        "\n",
        "Set up Friendli AI and get a token\n",
        "\n",
        "1.   Head to [FriendliAI](https://friendli.ai/get-started/serverless-endpoints), and create an account.\n",
        "2.   Grab a [**`FRIENDLI_TOKEN`**](https://friendli.ai/suite/setting/tokens) to use Friendli Serverless Endpoints for LLM calls."
      ],
      "metadata": {
        "id": "j-p7QeaUeo16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"FRIENDLI_TOKEN\"):\n",
        "    os.environ[\"FRIENDLI_TOKEN\"] = getpass.getpass(\"Enter your Friendli Token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ3S6NSQdsxZ",
        "outputId": "7ed02560-c5d1-4ff1-aceb-210ae45635c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Friendli Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traces will now be automatically logged to the Opik UI where you can inspect the inputs, outputs, and configure evaluation metrics. After you run this cell, follow the link to the Comet UI to see you traces."
      ],
      "metadata": {
        "id": "fuy9Dmg01s9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Weaviate Client\n",
        "\n",
        "Weaviate is a vector database which supports billion scale vector search with sub 50ms query times. We'll use Weaviate to query for books in this example."
      ],
      "metadata": {
        "id": "LCSQN2_x0-sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U weaviate-client --quiet"
      ],
      "metadata": {
        "id": "iGW4xKQDrTou",
        "outputId": "1c3ce21e-2eda-4ed6-8335-41fd76d068e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m419.8/437.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.0/437.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/223.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "from weaviate.classes.init import AdditionalConfig, Timeout\n",
        "\n",
        "\n",
        "WEAVIATE_CLUSTER_URL = os.getenv('WEAVIATE_CLUSTER_URL') or 'https://zxzyqcyksbw7ozpm5yowa.c0.us-west2.gcp.weaviate.cloud'\n",
        "WEAVIATE_API_KEY = os.getenv('WEAVIATE_API_KEY') or 'n6mdfI32xrXF3DH76i8Pwc2IajzLZop2igb6' # This is a read key\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WEAVIATE_CLUSTER_URL,\n",
        "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),\n",
        "    headers={\"X-Friendli-Token\": os.getenv('FRIENDLI_TOKEN')},\n",
        ")\n",
        "\n",
        "print(weaviate_client.is_connected())\n",
        "\n",
        "book_collection = weaviate_client.collections.get(name=\"WeaviateEmbeddingBooks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_tbFISQ0-gr",
        "outputId": "08aecdda-fa1c-4ffa-9a94-1f8223821c0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a RAG app with Friendli, Weaviate and Opik Traces\n",
        "\n",
        "Next, we will build a very simple LLM reasoning application and log the trace data to Opik where we can apply additional evaluation metrics and debug the LLM response.\n",
        "\n",
        "We will use FriendliAI as our inference provider to get fast, low-cost results from open source models. In this example, we're using Friendli's serverless endpoints, which require no infrastructure setup and are ideal for quick prototyping and experimentation. Just provide the API URL endpoint as https://api.friendli.ai/serverless/v1.\n",
        "\n",
        "For production use or personal deployments of custom models, [Friendli Dedicated Endpoints](https://friendli.ai/products/dedicated-endpoints) offers personal deployments of over 100k models on Hugging Face.\n",
        "\n",
        "We will use Opik to collect traces to inspect the inputs and outputs of the reasoning tasks, and to create evaluation metrics for hallicinations and other common or custom issues you want to detect.\n",
        "\n",
        "Opik integrates with OpenAI to provide a simple way to log traces for all OpenAI LLM calls. This works for all OpenAI models, including if you are using the streaming API."
      ],
      "metadata": {
        "id": "POkdxVeQcOvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.integrations.openai import track_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"rag-project\" #name your project. This will appear as the project name in the Opik UI\n",
        "\n",
        "\n",
        "friendli_client = OpenAI(\n",
        "    base_url=\"https://api.friendli.ai/serverless/v1\",\n",
        "    api_key=os.getenv('FRIENDLI_TOKEN')\n",
        ")\n",
        "\n",
        "@opik.track\n",
        "def call_llm(client, messages):\n",
        "    response = friendli_client.chat.completions.create(\n",
        "      model=\"meta-llama-3.3-70b-instruct\",\n",
        "      messages=messages\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "ciD73aK2qDXc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = input(\"What would you like to query for in the BookRecs dataset? \")\n",
        "\n",
        "response = book_collection.query.near_text(\n",
        "        query=user_query,\n",
        "        limit=3\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3nfojb5qj5K",
        "outputId": "33c8f9ba-f63b-466d-9dc0-f2a56730324a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What would you like to query for in the BookRecs dataset? Non fiction books\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for book in response.objects:\n",
        "    print(book.properties['title'])"
      ],
      "metadata": {
        "id": "B6lZZeyU59PU",
        "outputId": "b601b7ae-dbc4-498c-abe2-a522f9af465f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Fiction\n",
            "The Puffin Book of Nonsense Verse\n",
            "Species of Spaces and Other Pieces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the @opik.track decorator and the OpenAI logging integration to automatically log our traces and spans. Learn more here https://www.comet.com/docs/opik/tracing/log_traces#using-an-integration"
      ],
      "metadata": {
        "id": "NYUzfsnvY7JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@opik.track\n",
        "def retrieve_context(user_query):\n",
        "    # Semantic Search\n",
        "    response = book_collection.query.near_text(\n",
        "        query=user_query,\n",
        "        limit=3\n",
        "    )\n",
        "\n",
        "    recommended_books = []\n",
        "    for book in response.objects:\n",
        "        recommended_books.append(book.properties['title'])\n",
        "    return recommended_books"
      ],
      "metadata": {
        "id": "hbxVyTwdYC4R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@opik.track\n",
        "def generate_response(user_query, recommended_books):\n",
        "  prompt = f\"\"\"\n",
        "  You're a helpful assistant, reply to a chatbot message for someone inquiring for\n",
        "  book recommendations. The user query was {user_query}\n",
        "\n",
        "\n",
        "  These were the book that were extracted from the vector\n",
        "  search:\n",
        "\n",
        "  {recommended_books}\n",
        "  \"\"\"\n",
        "\n",
        "  messages=[\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt\n",
        "      }\n",
        "  ]\n",
        "  response = call_llm(friendli_client, messages)\n",
        "\n",
        "\n",
        "  return (response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "T4jYELuJ1Zjo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@opik.track(name=\"rag-example\")\n",
        "def llm_chain(user_query):\n",
        "    context = retrieve_context(user_query)\n",
        "    response = generate_response(user_query, context)\n",
        "    return response"
      ],
      "metadata": {
        "id": "-LhqAN_y2xS-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the LLM chain\n",
        "user_query = input(\"What types of books are you looking for? \")\n",
        "result = llm_chain(user_query)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "2fozL4CdW6lO",
        "outputId": "457384b6-e63c-4b38-a91d-30751c33f1bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What types of books are you looking for? movies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Started logging traces to the \"rag-project\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0196f008-7e25-7f39-aad8-0746e0c39f38&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems like you were looking for movie recommendations, but we found some great book titles that might interest you instead. If you're open to exploring some book options, we found:\n",
            "\n",
            "1. 'For Keeps' - a romantic novel that might have a cinematic feel to it.\n",
            "2. 'The Lord of the Rings' - a classic fantasy series that was actually adapted into a movie trilogy, so you might enjoy the book version.\n",
            "3. 'The Art of Alfred Hitchcock' - a non-fiction book about the legendary film director, which could give you insights into the world of movies.\n",
            "\n",
            "However, if you'd still like some movie recommendations, please let me know what type of movies you're in the mood for (e.g. action, comedy, horror, etc.) and I'd be happy to provide some suggestions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2vtOE4DDcmcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}